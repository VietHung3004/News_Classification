import pandas as pd
from underthesea import word_tokenize
from multiprocessing import Pool, cpu_count
from tqdm import tqdm
import time

# H√†m x·ª≠ l√Ω cho 1 item: input = (index, text) -> tr·∫£ v·ªÅ (index, tokenized_text)
def tokenize_item(item):
    idx, text = item
    tokens = word_tokenize(str(text), format="text")  # √©p v·ªÅ str ƒë·ªÉ tr√°nh l·ªói NaN
    return idx, tokens

if __name__ == "__main__":
    start = time.time()

    # 1) ƒê·ªçc CSV g·ªëc
    df = pd.read_csv("Cleaned_news_dataset.csv", encoding="utf-8")   # ho·∫∑c "utf-8-sig"
    n = len(df)
    print(f"üìä T·ªïng {n} d√≤ng c·∫ßn x·ª≠ l√Ω.")

    # 2) S·ªë core ƒë·ªÉ d√πng (ch·ª´a 2 core cho h·ªá ƒëi·ªÅu h√†nh)
    num_cores = max(1, cpu_count() - 1)
    print(f"‚ö° D√πng {num_cores}/{cpu_count()} core ƒë·ªÉ x·ª≠ l√Ω song song...")

    # 3) Chu·∫©n b·ªã d·ªØ li·ªáu (index, text)
    items = list(enumerate(df["clean_text"].astype(str)))

    # 4) M·∫£ng k·∫øt qu·∫£ t·∫°m
    results = [None] * n

    # 5) chunksize t·ª± ƒë·ªông (to√†n dataset chia ƒë·ªÅu cho s·ªë core * 10)
    chunksize = max(1, n // (num_cores * 10))
    print(f"‚öôÔ∏è chunksize = {chunksize}")

    # 6) Tokenize song song v·ªõi progress bar
    with Pool(processes=num_cores) as pool:
        for idx, tokenized in tqdm(pool.imap_unordered(tokenize_item, items, chunksize=chunksize), total=n):
            results[idx] = tokenized

    # 7) G·∫Øn k·∫øt qu·∫£ v√†o DataFrame
    df["tokens"] = results
    df = df.drop(columns=["clean_text"])  # b·ªè c·ªôt g·ªëc n·∫øu kh√¥ng c·∫ßn
    # 8) Xu·∫•t ra CSV (utf-8-sig ƒë·ªÉ Excel ƒë·ªçc ti·∫øng Vi·ªát kh√¥ng l·ªói)
    df.to_csv("Data_tokenized.csv", index=False, encoding="utf-8-sig")

    print(f"‚úÖ Xong! File CSV l∆∞u t·∫°i: data_tokenized.csv")
    print(f"‚è±Ô∏è T·ªïng th·ªùi gian: {time.time() - start:.2f} gi√¢y")